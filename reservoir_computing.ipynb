{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTUUMJogDhEzPPgyXlzUE9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reservoir Computing**\n",
        "\n",
        "Introduced by Jaeger and Haas (2004), a reservoir computer is a type of Recurrent Neural Network (RNN). However, unlike traditional RNNs, an RC is initialized randomly and remains unaltered throughout training. This means it does not necessarily undergo backpropagation or gradient descent.\n",
        "\n",
        "RC primarily focuses on its output layer. The outputs from the reservoir nodes are linearly aggregated to yield the expected outcome, making the training process similar to that of a linear regression model.\n",
        "\n",
        "The readout layer (or output) weights are fine-tuned to minimize the discrepancy between the RC's predictions and the actual outputs.\n",
        "\n",
        "**Mathematically:**\n",
        "\n",
        "Let $u(t)$ be an input vector that is fed into a reservoir $R$ through an input (Reservoir coupler). The input to $R$ then becomes,\n",
        "\n",
        "$W_{in} u(t)$\n",
        "\n",
        "Where\n",
        "\n",
        "*  $W_{in} \\in ℝ ^{D_r×D} $\n",
        "*  $D_r$ is the dimension of the reservoir, and\n",
        "*  $D$ is the dimension of the input.\n",
        "\n",
        "In $R$ the input is then combined with the current reservoir state, $r(t)$ to produce the output (or the next reservoir state) $r(t+dt)$.\n",
        "\n",
        "The reservoir is represented by an adjacency matrix *A* where:\n",
        "\n",
        "*  $A ∈ ℝ^{D_r×D}$ and\n",
        "*  $D_r$ is the dimension of the reservoir.\n",
        "\n",
        "Therefore:\n",
        "\n",
        "$r(t+dt) = tanh[Ar(t)+W_{in}u(t)]$\n",
        "\n",
        "Each reservoir node has a corresponding reservoir state, and $r(t+dt)$ flows into the reservoir.\n",
        "\n",
        "Here, a mapping is done, given by:\n",
        "\n",
        "$v(t+dt) = r(t+dt), W_{out})$\n",
        "\n",
        "Where:\n",
        "\n",
        "*  $v(t+dt) ∈ ℝ^D $, and\n",
        "*  $W_{out} ∈ ℝ^{D×D_r}$\n",
        "\n",
        "**The goal is for the output $v(t)$ to approximate the actual desired output $v'(t)$.**\n",
        "\n",
        "During the training process $(-T ≤ t < 0)$. An input $u(t)$ is fed into the reservoir, and the resulting reservoir state $r(t)$ is then stored along with $u(t)$ as training data.\n",
        "\n",
        "**The parameters of $W_{out}$ are then trained to minimize the mean squared difference between $v(t)$ and $v'(t)$** given by:\n",
        "\n",
        "$∑_{t=t_0}^{T}||(r(t),W_{out})-v'_d(t)||^2+ β ||W_{out}||^2$\n",
        "\n",
        "Where beta corresponds to the regularization constant and prevents overfitting if $β>0$.\n",
        "\n",
        "To transition from a training phase to the prediction phase, we let $v(t+dt)=u(t+dt)$. This corresponds to letting the output vector $v(t+dt)$ serve as the input for the next timestamp $u(t+dt)$.\n",
        "\n",
        "The neural network is then allowed to run autonomously according to:\n",
        "\n",
        "$r(t+dt) =tanh [Ar(t) +W_{in} (r(t), W_{out})]$\n",
        "\n",
        "Therefore, the output of the \"trained\" RC gives the input (predicted value) for the next timestep $u(t)$ for $t>0$."
      ],
      "metadata": {
        "id": "0K4dtSvKuhb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the Activation Functions**"
      ],
      "metadata": {
        "id": "4JU-VafBnxV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Traditionanal Functions**"
      ],
      "metadata": {
        "id": "FPF22oJrt_Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Sigmoid activation function that transforms input values to range (0,1).\n",
        "\n",
        "  Parameters:\n",
        "    x: Input array or value\n",
        "\n",
        "  Returns:\n",
        "    Sigmoid transformation of input\n",
        "  \"\"\"\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"\n",
        "    Hyperbolic tangent activation function that transforms input values to range (-1,1).\n",
        "\n",
        "    Parameters:\n",
        "        x: Input array or value\n",
        "\n",
        "    Returns:\n",
        "        Hyperbolic tangent transformation of input\n",
        "    \"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Rectified Linear Unit (ReLU) activation function.\n",
        "    Returns x if x > 0, otherwise returns 0.\n",
        "\n",
        "    Parameters:\n",
        "        x: Input array or value\n",
        "\n",
        "    Returns:\n",
        "        ReLU transformation of input\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)"
      ],
      "metadata": {
        "id": "GKH6sBzxn2yC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Adapted ReLU Functions**"
      ],
      "metadata": {
        "id": "WOexzByVuD7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    \"\"\"\n",
        "    Leaky ReLU activation function.\n",
        "    Returns x if x > 0, otherwise returns alpha * x.\n",
        "\n",
        "    Parameters:\n",
        "        x: Input array or value\n",
        "        alpha: Slope for negative values (default: 0.01)\n",
        "\n",
        "    Returns:\n",
        "        Leaky ReLU transformation of input\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "def elu(x, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Exponential Linear Unit (ELU) activation function.\n",
        "    Returns x if x > 0, otherwise returns alpha * (exp(x) - 1).\n",
        "\n",
        "    Parameters:\n",
        "        x: Input array or value\n",
        "        alpha: Scaling factor for negative values (default: 1.0)\n",
        "\n",
        "    Returns:\n",
        "        ELU transformation of input\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
        "\n",
        "def prelu(x, alpha):\n",
        "    \"\"\"\n",
        "    Parametric ReLU (PReLU) activation function.\n",
        "    Returns x if x > 0, otherwise returns alpha * x.\n",
        "    The alpha parameter is learned during training.\n",
        "\n",
        "    Parameters:\n",
        "        x: Input array or value\n",
        "        alpha: Learned parameter array with same shape as x\n",
        "\n",
        "    Returns:\n",
        "        PReLU transformation of input\n",
        "    \"\"\"\n",
        "    return np.where(x > 0, x, alpha * x)"
      ],
      "metadata": {
        "id": "gUMbb7u2uEqL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define Helper Functions**"
      ],
      "metadata": {
        "id": "9nRr-qp_oh10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import linalg\n",
        "\n",
        "def linear_regression(X, Y):\n",
        "    \"\"\"\n",
        "    Computes linear regression weights for mapping reservoir states to target outputs.\n",
        "    Solves the equation W_out * X = Y for W_out using least squares.\n",
        "\n",
        "    Parameters:\n",
        "        X: Matrix of reservoir states (dim_reservoir × n_samples)\n",
        "        Y: Matrix of target outputs (n_samples × dim_system)\n",
        "\n",
        "    Returns:\n",
        "        W_out: Output weight matrix (dim_system × dim_reservoir)\n",
        "    \"\"\"\n",
        "    # Transpose X to match the dimensions expected in the solve\n",
        "    X_T = X.T\n",
        "\n",
        "    # Solve the linear system for W_out.T\n",
        "    # We're solving X_T @ W_out.T = Y\n",
        "    # Using numpy's least squares solver for better numerical stability\n",
        "    W_out_T = linalg.lstsq(X_T, Y)[0]\n",
        "\n",
        "    # Return the transpose to get W_out\n",
        "    return W_out_T.T\n",
        "\n",
        "def generate_adjacency_matrix(dim_reservoir, rho, sigma, density=0.1):\n",
        "    \"\"\"\n",
        "    Generates a random adjacency matrix for the reservoir.\n",
        "\n",
        "    Parameters:\n",
        "        dim_reservoir: Size of the reservoir\n",
        "        rho: Spectral radius - controls the dynamics of the reservoir\n",
        "        sigma: Scaling factor for weights\n",
        "        density: Fraction of non-zero connections (default 0.1)\n",
        "\n",
        "    Returns:\n",
        "        A: Adjacency matrix for the reservoir\n",
        "    \"\"\"\n",
        "    # Generate a sparse random matrix\n",
        "    A = np.random.rand(dim_reservoir, dim_reservoir)\n",
        "\n",
        "    # Apply sparsity by setting values below threshold to zero\n",
        "    A[A < (1 - density)] = 0\n",
        "\n",
        "    # Scale the matrix\n",
        "    A = A * 2 * sigma - sigma\n",
        "\n",
        "    # Use the scale_matrix function instead of direct scaling\n",
        "    A = scale_matrix(A, rho)\n",
        "\n",
        "    return A\n",
        "\n",
        "def scale_matrix(A, rho):\n",
        "    eigenvalues, _ = np.linalg.eig(A)\n",
        "    max_eigenvalue = np.amax(eigenvalues)\n",
        "    A = A/np.absolute(max_eigenvalue) * rho\n",
        "    return A"
      ],
      "metadata": {
        "id": "wLy9dRZ1omTU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define Reservoir Computer Class**"
      ],
      "metadata": {
        "id": "ShXsGcOihw2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hwv--ncg7mH",
        "outputId": "e6acc9b3-d564-4803-c92d-3b7a089089bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QamGUGTjfN2Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReservoirComputer:\n",
        "  def __init__ (self, dim_system, dim_reservoir, rho, sigma, density,\n",
        "                activation_function):\n",
        "        # dim_system = dimension of the input / output system\n",
        "        # dim_reservoir = size of the reservoir (number of neurons)\n",
        "        # rho = spectral radius parameter that affects the dynamics\n",
        "        # sigma = scale parameter for weight initialization\n",
        "        # density = controls the sparsity of the reservoir connections\n",
        "        # activation_function = the activation function; sigmoid = default\n",
        "    self.dim_system = dim_system\n",
        "        # stores the system dimensions as variables\n",
        "    self.dim_reservoir = dim_reservoir\n",
        "        # stores the reservoir dimensions as variables\n",
        "    self.r_state = np.zeros(dim_reservoir)\n",
        "        # initializes the reservoir state as a vector of zeros with length equal\n",
        "        # to the reservoir dimension\n",
        "    self.A = generate_adjacency_matrix(dim_reservoir, rho, sigma, density)\n",
        "        # creates the reservoir's internal connection matrix using a function\n",
        "    self.W_in = 2 * sigma * (np.random.rand(dim_reservoir, dim_system) - 0.5)\n",
        "        # creates the input weight matrix (connecting inputs to the reservoir)\n",
        "        # with random values between -σ and σ\n",
        "    self.W_out = 2 * sigma * (np.random.rand(dim_system, dim_reservoir))\n",
        "        # initializes an output weight matrix (connecting the reservoir to the output)\n",
        "        # with random values between 0 and 2σ\n",
        "    self.activation_function = activation_function #store the activation function\n",
        "\n",
        "  def advance_r_state(self, u):\n",
        "        # updates the reservoir state based on the current input u\n",
        "    self.r_state = self.activation_function(np.dot(self.A, self.r_state) + np.dot(\n",
        "        self.W_in, u))\n",
        "        # updates the reservoir state using:\n",
        "          # the current state multiplied by the adjacency matrix\n",
        "          # plus the input weighted by the input matrix\n",
        "          # then applies the sigmoid activation function\n",
        "            # use any activation function here - sigmoid is a placeholder\n",
        "    return self.r_state\n",
        "        # returns the new reservoir state\n",
        "\n",
        "  def v(self):\n",
        "        # computes the output of the reservoir computer\n",
        "    return np.dot(self.W_out, self.r_state)\n",
        "        # the output is calculated by multiplying the output weight matrix with\n",
        "        # the current reservoir state\n",
        "\n",
        "  def train(self, trajectory):\n",
        "    # trajectory has shape (n, dim_system), n is the number of timesteps\n",
        "        # so... the parameter is a 2D array with shape (n, dim_system), where n is\n",
        "        # the number of time steps\n",
        "    R = np.zeros((self.dim_reservoir, trajectory.shape[0]))\n",
        "        # creates matrix R to store the reservoir states for each trajectory timestep\n",
        "    for i in range(trajectory.shape[0]):\n",
        "        # loops through each timestep in the trajectory\n",
        "      R[:, i] = self.r_state\n",
        "        # stores the current reservoir state in the i-th comumn of R\n",
        "      u = trajectory[i]\n",
        "        # gets the current input from the trajectory\n",
        "      self.advance_r_state(u)\n",
        "        # updates the reservoir state using the current input\n",
        "    self.W_out = linear_regression(R, trajectory)\n",
        "        # computes the output weights using linear regression between the collected\n",
        "        # reservoir states (R) and the target trajectory\n",
        "\n",
        "  def predict(self, steps):\n",
        "    prediction = np.zeros((steps, self.dim_system))\n",
        "        # creates an array to store the predictioons with shape (steps, dim_system)\n",
        "    for i in range(steps):\n",
        "        # loops through each prediction step\n",
        "        v = self.v() # computes the current output in the prediction array\n",
        "        prediction[i] = v # updates the reservoir state using the current output\n",
        "          # (feeding back the prediction)\n",
        "        self.advance_r_state(v)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "UgcDerjJfnie"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot the Results**\n",
        "\n",
        "**(Lorenz-63 Attractor)**"
      ],
      "metadata": {
        "id": "FXWByx1kiKPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Import for 3D plotting\n",
        "\n",
        "# Assuming valid_data and pred_data are already defined\n",
        "# Example (remove if you already have the data):\n",
        "# valid_data = np.array(...)  # Ground truth data\n",
        "# pred_data = np.array(...)   # Prediction data\n",
        "\n",
        "# Create a 3D figure\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Extract coordinates from the arrays\n",
        "x_data = valid_data[:, 0]\n",
        "y_data = valid_data[:, 1]\n",
        "z_data = valid_data[:, 2]\n",
        "\n",
        "x_pred = pred_data[:, 0]\n",
        "y_pred = pred_data[:, 1]\n",
        "z_pred = pred_data[:, 2]\n",
        "\n",
        "# Plot the actual trajectory\n",
        "ax.plot(x_data, y_data, z_data, lw=0.5, label=\"Actual\", color='blue')\n",
        "\n",
        "# Plot the predicted trajectory\n",
        "ax.plot(x_pred, y_pred, z_pred, lw=0.5, label=\"Predicted\", color='red')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel(\"X Axis\")\n",
        "ax.set_ylabel(\"Y Axis\")\n",
        "ax.set_zlabel(\"Z Axis\")\n",
        "\n",
        "# Add legend and title\n",
        "ax.legend()\n",
        "ax.set_title(\"Lorenz Attractor\")\n",
        "\n",
        "# Adjust the view angle for better visualization\n",
        "ax.view_init(elev=30, azim=45)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YjBDtp2NsROX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicted Data vs Actual Data as a Function of Time**"
      ],
      "metadata": {
        "id": "umLoylvFsv3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming these variables are already defined:\n",
        "# training_data, data_length, x_data, y_data, z_data, x_pred, y_pred, z_pred\n",
        "\n",
        "# Create a figure with 3 subplots stacked vertically\n",
        "fig, axs = plt.subplots(3, 1, figsize=(8, 10))\n",
        "\n",
        "# Define the timesteps for the x-axis\n",
        "timesteps = range(training_data.shape[0], data_length)\n",
        "\n",
        "# Plot X dimension comparison\n",
        "axs[0].plot(timesteps, x_data, label=\"x\", lw=1.1, color='blue')\n",
        "axs[0].plot(timesteps, x_pred, label=\"x_pred\", color='orange', lw=1.1)\n",
        "axs[0].set_ylabel(\"x\")\n",
        "axs[0].set_title(\"Comparison of the Predicted Lorenz Data and Actual Data\")\n",
        "axs[0].legend()\n",
        "\n",
        "# Plot Y dimension comparison\n",
        "axs[1].plot(timesteps, y_data, label=\"y\", color='r', lw=1.1)\n",
        "axs[1].plot(timesteps, y_pred, label=\"y_pred\", color='orange', lw=1.1)\n",
        "axs[1].set_ylabel(\"y\")\n",
        "axs[1].legend()\n",
        "\n",
        "# Plot Z dimension comparison\n",
        "axs[2].plot(timesteps, z_data, label=\"z\", color='g', lw=0.9)\n",
        "axs[2].plot(timesteps, z_pred, label=\"z_pred\", color='orange', lw=1.1)\n",
        "axs[2].set_ylabel(\"z\")\n",
        "axs[2].set_xlabel(\"timestep\")\n",
        "axs[2].legend()\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "fig.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0waXY26UrU9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}